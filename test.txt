============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.4, pluggy-1.5.0
rootdir: /home/kasinadhsarma/experiment/cognition-l1-experiment
configfile: pytest.ini
testpaths: tests
plugins: cov-6.0.0, anyio-4.7.0
collected 48 items

tests/benchmarks/test_arc_reasoning.py FFF                               [  6%]
tests/benchmarks/test_bigbench_reasoning.py FFF                          [ 12%]
tests/test_consciousness.py .F.                                          [ 18%]
tests/test_environment.py .....                                          [ 29%]
tests/unit/attention/test_attention.py ..F.                              [ 37%]
tests/unit/attention/test_attention_mechanisms.py ....                   [ 45%]
tests/unit/integration/test_cognitive_integration.py FFFF                [ 54%]
tests/unit/integration/test_state_management.py .FF.                     [ 62%]
tests/unit/memory/test_integration.py FFFF                               [ 70%]
tests/unit/memory/test_memory.py ..FF                                    [ 79%]
tests/unit/memory/test_memory_components.py FFFFF                        [ 89%]
tests/unit/state/test_consciousness_state_management.py .....            [100%]

=================================== FAILURES ===================================
__________________ TestARCReasoning.test_pattern_recognition ___________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_arc_reasoning.TestARCReasoning object at 0x776c9d437430>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_pattern_recognition(self, key, consciousness_model):
        inputs, expected = self.load_arc_sample()
        batch_size = inputs['visual'].shape[0]
    
        # Initialize model state
        model_inputs = {
            'visual': inputs['visual'],
            'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))
        }
    
        # Initialize model
>       variables = consciousness_model.init(key, model_inputs)

tests/benchmarks/test_arc_reasoning.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:101: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 512
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 512, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
_________________ TestARCReasoning.test_abstraction_capability _________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_arc_reasoning.TestARCReasoning object at 0x776c9d434d00>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_abstraction_capability(self, key, consciousness_model):
        inputs, _ = self.load_arc_sample()
        batch_size = inputs['visual'].shape[0]
    
        # Create transformed versions
        variations = {
            'original': inputs['visual'],
            'rotated': jnp.rot90(inputs['visual'][:, :, :, 0], k=1)[:, :, None],
            'scaled': inputs['visual'] * 2.0
        }
    
        try:
>           variables = consciousness_model.init(
                key,
                {'visual': variations['original'],
                 'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))}
            )

tests/benchmarks/test_arc_reasoning.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:101: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 512
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 512, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError

During handling of the above exception, another exception occurred:

self = <tests.benchmarks.test_arc_reasoning.TestARCReasoning object at 0x776c9d434d00>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_abstraction_capability(self, key, consciousness_model):
        inputs, _ = self.load_arc_sample()
        batch_size = inputs['visual'].shape[0]
    
        # Create transformed versions
        variations = {
            'original': inputs['visual'],
            'rotated': jnp.rot90(inputs['visual'][:, :, :, 0], k=1)[:, :, None],
            'scaled': inputs['visual'] * 2.0
        }
    
        try:
            variables = consciousness_model.init(
                key,
                {'visual': variations['original'],
                 'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))}
            )
    
            states = {}
            for name, visual_input in variations.items():
                output, metrics = consciousness_model.apply(
                    variables,
                    {'visual': visual_input,
                     'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))},
                    deterministic=True
                )
                states[name] = output
    
            # Test representation similarity
            def cosine_similarity(x, y):
                return jnp.sum(x * y) / (jnp.linalg.norm(x) * jnp.linalg.norm(y))
    
            orig_rot_sim = cosine_similarity(
                states['original'].ravel(),
                states['rotated'].ravel()
            )
            orig_scaled_sim = cosine_similarity(
                states['original'].ravel(),
                states['scaled'].ravel()
            )
    
            # Transformed versions should maintain similar representations
            assert orig_rot_sim > 0.5
            assert orig_scaled_sim > 0.7
    
        except Exception as e:
>           pytest.fail(f"Abstraction capability test failed: {str(e)}")
E           Failed: Abstraction capability test failed: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

tests/benchmarks/test_arc_reasoning.py:124: Failed
__________________ TestARCReasoning.test_conscious_adaptation __________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_arc_reasoning.TestARCReasoning object at 0x776c9d436dd0>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_conscious_adaptation(self, key, consciousness_model):
        inputs, _ = self.load_arc_sample()
        batch_size = inputs['visual'].shape[0]
    
        try:
            # Create simple and complex patterns
            simple_input = {
                'visual': inputs['visual'],
                'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))
            }
    
            # More complex pattern (doubled size)
            complex_visual = jnp.tile(inputs['visual'], (1, 2, 2, 1))
            complex_input = {
                'visual': complex_visual,
                'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))
            }
    
>           variables = consciousness_model.init(key, simple_input)

tests/benchmarks/test_arc_reasoning.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:101: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 512
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 512, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError

During handling of the above exception, another exception occurred:

self = <tests.benchmarks.test_arc_reasoning.TestARCReasoning object at 0x776c9d436dd0>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_conscious_adaptation(self, key, consciousness_model):
        inputs, _ = self.load_arc_sample()
        batch_size = inputs['visual'].shape[0]
    
        try:
            # Create simple and complex patterns
            simple_input = {
                'visual': inputs['visual'],
                'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))
            }
    
            # More complex pattern (doubled size)
            complex_visual = jnp.tile(inputs['visual'], (1, 2, 2, 1))
            complex_input = {
                'visual': complex_visual,
                'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))
            }
    
            variables = consciousness_model.init(key, simple_input)
    
            # Process both patterns
            _, simple_metrics = consciousness_model.apply(
                variables,
                simple_input,
                deterministic=True
            )
    
            _, complex_metrics = consciousness_model.apply(
                variables,
                complex_input,
                deterministic=True
            )
    
            # Validate complexity adaptation
            assert complex_metrics['phi'] > simple_metrics['phi']
            assert 'attention_weights' in simple_metrics
            assert 'attention_weights' in complex_metrics
    
        except Exception as e:
>           pytest.fail(f"Conscious adaptation test failed: {str(e)}")
E           Failed: Conscious adaptation test failed: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

tests/benchmarks/test_arc_reasoning.py:165: Failed
______________ TestBigBenchReasoning.test_reasoning_capabilities _______________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_bigbench_reasoning.TestBigBenchReasoning object at 0x776c9d4374f0>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_reasoning_capabilities(self, key, consciousness_model):
        tasks = self.load_sample_tasks()
>       variables = consciousness_model.init(key, {'textual': jnp.zeros((1, 1, 512))})

tests/benchmarks/test_bigbench_reasoning.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:101: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 512
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 512, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
___________________ TestBigBenchReasoning.test_meta_learning ___________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_bigbench_reasoning.TestBigBenchReasoning object at 0x776c9d436440>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_meta_learning(self, key, consciousness_model):
        """Test model's ability to adapt to new reasoning patterns."""
        # Create sequence of related but progressively complex tasks
        sequence = [
            {'textual': "1, 2, 3, _", 'expected': "4"},
            {'textual': "2, 4, 6, _", 'expected': "8"},
            {'textual': "3, 6, 9, _", 'expected': "12"}
        ]
    
>       variables = consciousness_model.init(
            key,
            {'textual': jnp.zeros((1, 1, 512))}
        )

tests/benchmarks/test_bigbench_reasoning.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:101: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 512
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 512, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
______________ TestBigBenchReasoning.test_consciousness_emergence ______________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_bigbench_reasoning.TestBigBenchReasoning object at 0x776c9d436770>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_consciousness_emergence(self, key, consciousness_model):
        """
        Test for emergence of consciousness-like behaviors:
        1. Integration of information
        2. Adaptive processing
        3. Self-monitoring
        """
        # Complex multi-step reasoning task
        task_embedding = random.normal(key, (1, 128, 512))
>       variables = consciousness_model.init(
            key,
            {'textual': task_embedding}
        )

tests/benchmarks/test_bigbench_reasoning.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:101: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 512
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 512, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
________________ TestConsciousnessModel.test_model_forward_pass ________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.test_consciousness.TestConsciousnessModel object at 0x776c9d4683a0>
model = ConsciousnessModel(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 4
    num_states = 4
    dropout_rate = 0.1
)
sample_input = {'attention': Array([[[-0.02862089,  1.5240539 , -1.0556508 , ..., -0.27188757,
         -0.88195777,  0.11891642],
  ...        [-0.5756394 , -0.20118208, -0.08988765, ...,  0.23238769,
          1.5470275 , -1.2839596 ]]], dtype=float32)}
key = Array([ 0, 42], dtype=uint32), deterministic = True

    def test_model_forward_pass(self, model, sample_input, key, deterministic):
        """Test forward pass through consciousness model."""
        # Initialize model
>       variables = model.init(key, sample_input, deterministic=deterministic)

tests/test_consciousness.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:101: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 64
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 64, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
____________ TestConsciousnessAttention.test_attention_output_shape ____________

self = <test_attention.TestConsciousnessAttention object at 0x776c9d469e10>

    def test_attention_output_shape(self):
        batch_size = 2
        seq_length = 8
        input_dim = 128
    
>       inputs_q = random.normal(key, (batch_size, seq_length, input_dim))
E       NameError: name 'key' is not defined

tests/unit/attention/test_attention.py:130: NameError
__________ TestCognitiveProcessIntegration.test_cross_modal_attention __________

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x776c9d46b940>
key = Array([0, 0], dtype=uint32)
integration_module = CognitiveProcessIntegration(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 3
    dropout_rate = 0.1
)

    def test_cross_modal_attention(self, key, integration_module):
        # Test dimensions
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Create multi-modal inputs
        inputs = {
            'visual': random.normal(key, (batch_size, seq_length, input_dim)),
            'textual': random.normal(
                random.PRNGKey(1),
                (batch_size, seq_length, input_dim)
            ),
            'numerical': random.normal(
                random.PRNGKey(2),
                (batch_size, seq_length, input_dim)
            )
        }
    
        # Initialize parameters
        variables = integration_module.init(key, inputs)
    
        # Process through integration
        consciousness_state, attention_maps = integration_module.apply(
            variables,
            inputs,
            deterministic=True
        )
    
        # Test output shapes
        assert consciousness_state.shape == (batch_size, seq_length, 64)
    
        # Test attention maps
        for source in inputs.keys():
            for target in inputs.keys():
                if source != target:
                    map_key = f"{target}-{source}"
>                   assert map_key in attention_maps
E                   AssertionError: assert 'textual-visual' in {}

tests/unit/integration/test_cognitive_integration.py:63: AssertionError
______ TestCognitiveProcessIntegration.test_modality_specific_processing _______
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x776c9d46bb20>
key = Array([0, 0], dtype=uint32)
integration_module = CognitiveProcessIntegration(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 3
    dropout_rate = 0.1
)

    def test_modality_specific_processing(self, key, integration_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Test with single modality
        single_input = {
            'visual': random.normal(key, (batch_size, seq_length, input_dim))
        }
        variables = integration_module.init(key, single_input)
    
        consciousness_state1, _ = integration_module.apply(
            variables,
            single_input,
            deterministic=True
        )
    
        # Test with multiple modalities
        multi_input = {
            'visual': single_input['visual'],
            'textual': random.normal(key, (batch_size, seq_length, input_dim))
        }
    
>       consciousness_state2, _ = integration_module.apply(
            variables,
            multi_input,
            deterministic=True
        )

tests/unit/integration/test_cognitive_integration.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_state.py:21: in __call__
    x = nn.LayerNorm()(x)
../../.local/lib/python3.10/site-packages/flax/linen/normalization.py:518: in __call__
    return _normalize(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

mdl = LayerNorm(
    # attributes
    epsilon = 1e-06
    dtype = None
    param_dtype = float32
    use_bias = True
    use...-1
    axis_name = None
    axis_index_groups = None
    use_fast_variance = True
    force_float32_reductions = True
)
x = Array([[[-2.0545669 ,  0.33981755,  0.2619878 , -0.9536857 ,
          1.1861376 ,  1.2227523 ,  0.1118008 , -0.327318...86593276,  0.17548549, -2.5559947 ,
          0.0899727 , -1.434366  ,  0.5375884 ,  1.2183367 ]]],      dtype=float32)
mean = Array([[[ 0.316849  ],
        [ 0.15681918],
        [-0.09266028],
        [-0.09894891],
        [-0.3263538 ],
   ...16475785],
        [-0.13104448],
        [ 0.06134801],
        [-0.21195135],
        [-0.32997662]]], dtype=float32)
var = Array([[[0.85883766],
        [0.66789114],
        [0.95650977],
        [1.002326  ],
        [0.5636929 ],
        ... [0.5845102 ],
        [1.0722553 ],
        [1.0433921 ],
        [0.83152187],
        [1.3724558 ]]], dtype=float32)
reduction_axes = (2,), feature_axes = (2,), dtype = None
param_dtype = <class 'jax.numpy.float32'>, epsilon = 1e-06, use_bias = True
use_scale = True, bias_init = <function zeros at 0x776c9e065480>
scale_init = <function ones at 0x776c9df00a60>, force_float32_reductions = True

    def _normalize(
      mdl: Module,
      x: Array,
      mean: Array,
      var: Array,
      reduction_axes: Axes,
      feature_axes: Axes,
      dtype: Dtype | None,
      param_dtype: Dtype,
      epsilon: float,
      use_bias: bool,
      use_scale: bool,
      bias_init: Initializer,
      scale_init: Initializer,
      force_float32_reductions: bool = True
    ):
      """Normalizes the input of a normalization layer and optionally applies a learned scale and bias.
    
      Arguments:
        mdl: Module to apply the normalization in (normalization params will reside
          in this module).
        x: The input.
        mean: Mean to use for normalization.
        var: Variance to use for normalization.
        reduction_axes: The axes in ``x`` to reduce.
        feature_axes: Axes containing features. A separate bias and scale is learned
          for each specified feature.
        dtype: The dtype of the result (default: infer from input and params).
        param_dtype: The dtype of the parameters.
        epsilon: Normalization epsilon.
        use_bias: If true, add a bias term to the output.
        use_scale: If true, scale the output.
        bias_init: Initialization function for the bias term.
        scale_init: Initialization function for the scaling function.
        force_float32_reductions: If false, the scale and bias parameters use the
          param_dtype. Otherwise, they will have at least float32 precision due to
          the mean and var being promoted to float32.
    
      Returns:
        The normalized input.
      """
      reduction_axes = _canonicalize_axes(x.ndim, reduction_axes)
      feature_axes = _canonicalize_axes(x.ndim, feature_axes)
      feature_shape = [1] * x.ndim
      reduced_feature_shape = []
      for ax in feature_axes:
        feature_shape[ax] = x.shape[ax]
        reduced_feature_shape.append(x.shape[ax])
    
      mean = jnp.expand_dims(mean, reduction_axes)
      var = jnp.expand_dims(var, reduction_axes)
      y = x - mean
      mul = lax.rsqrt(var + epsilon)
      args = [x]
      if use_scale:
>       scale = mdl.param(
          'scale', scale_init, reduced_feature_shape, param_dtype
        ).reshape(feature_shape)
E       flax.errors.ScopeParamNotFoundError: Could not find parameter named "scale" in scope "/LayerNorm_1". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamNotFoundError)

../../.local/lib/python3.10/site-packages/flax/linen/normalization.py:204: ScopeParamNotFoundError
__________ TestCognitiveProcessIntegration.test_integration_stability __________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x776c9d46bd00>
key = Array([0, 0], dtype=uint32)
integration_module = CognitiveProcessIntegration(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 3
    dropout_rate = 0.1
)

    def test_integration_stability(self, key, integration_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        inputs = {
            'modality1': random.normal(key, (batch_size, seq_length, input_dim)),
            'modality2': random.normal(key, (batch_size, seq_length, input_dim))
        }
    
        variables = integration_module.init(key, inputs)
    
        # Test stability across multiple forward passes
        states = []
        for _ in range(5):
            state, _ = integration_module.apply(
                variables,
                inputs,
                deterministic=True
            )
            states.append(state)
    
        # All forward passes should produce identical results
        for i in range(1, len(states)):
            assert jnp.allclose(states[0], states[i])
    
        # Test with dropout
        states_dropout = []
        for i in range(5):
>           state, _ = integration_module.apply(
                variables,
                inputs,
                deterministic=False,
                rngs={'dropout': random.PRNGKey(i)}
            )

tests/unit/integration/test_cognitive_integration.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 3
    dropout_rat...
        kernel_init = init
        bias_init = zeros
        dot_general = None
        dot_general_cls = None
    )
)
inputs = {'modality1': Array([[[-2.0545669 ,  0.33981755,  0.2619878 , -0.9536857 ,
          1.1861376 ,  1.2227523 ,  0.11180...6593276,  0.17548549, -2.5559947 ,
          0.0899727 , -1.434366  ,  0.5375884 ,  1.2183367 ]]],      dtype=float32)}
deterministic = False

    @nn.compact
    def __call__(self, inputs: Dict[str, jnp.ndarray], deterministic: bool = True):
        # Process each modality separately first
        processed_modalities = {}
        for modality, x in inputs.items():
            x = nn.LayerNorm()(x)
            x = nn.Dense(self.hidden_dim)(x)
            x = nn.gelu(x)
            if not deterministic:
>               x = nn.dropout(x, rate=self.dropout_rate, deterministic=deterministic)
E               AttributeError: module 'flax.linen' has no attribute 'dropout'. Did you mean: 'Dropout'?

models/consciousness_state.py:25: AttributeError
__________ TestCognitiveProcessIntegration.test_cognitive_integration __________

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x776c9d46beb0>

    def test_cognitive_integration(self):
        # Test dimensions
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Create multi-modal inputs
        inputs = {
            'visual': random.normal(random.PRNGKey(0), (batch_size, seq_length, input_dim)),
            'textual': random.normal(random.PRNGKey(1), (batch_size, seq_length, input_dim)),
            'numerical': random.normal(random.PRNGKey(2), (batch_size, seq_length, input_dim))
        }
    
        # Initialize parameters
>       variables = integration_module.init(random.PRNGKey(0), inputs)
E       NameError: name 'integration_module' is not defined

tests/unit/integration/test_cognitive_integration.py:162: NameError
______________ TestConsciousnessStateManager.test_rl_optimization ______________

self = <test_state_management.TestConsciousnessStateManager object at 0x776c9d4985e0>
key = Array([0, 0], dtype=uint32)
state_manager = ConsciousnessStateManager(
    # attributes
    hidden_dim = 64
    num_states = 4
    dropout_rate = 0.1
)

    def test_rl_optimization(self, key, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = random.normal(key, (batch_size, hidden_dim))
        inputs = random.normal(key, (batch_size, hidden_dim))
        variables = state_manager.init(key, state, inputs)
    
        # Get state values for current and next state
        new_state, metrics = state_manager.apply(
            variables,
            state,
            inputs,
            threshold=0.5,
            deterministic=True
        )
    
        # Test RL loss computation
        reward = jnp.ones((batch_size, 1))  # Mock reward
        value_loss, td_error = state_manager.apply(
            variables,
            method=state_manager.get_rl_loss,
            state_value=metrics['state_value'],
            reward=reward,
            next_state_value=metrics['state_value']
        )
    
        # Test loss properties
        assert jnp.isscalar(value_loss)
        assert value_loss >= 0.0
>       assert td_error.shape == (batch_size, 1)
E       assert (2, 2, 1) == (2, 1)
E         
E         At index 1 diff: 2 != 1
E         Left contains one more item: 1
E         Use -v to get more diff

tests/unit/integration/test_state_management.py:94: AssertionError
______________ TestConsciousnessStateManager.test_adaptive_gating ______________

self = <test_state_management.TestConsciousnessStateManager object at 0x776c9d4987c0>
key = Array([0, 0], dtype=uint32)
state_manager = ConsciousnessStateManager(
    # attributes
    hidden_dim = 64
    num_states = 4
    dropout_rate = 0.1
)

    def test_adaptive_gating(self, key, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = random.normal(key, (batch_size, hidden_dim))
        variables = state_manager.init(key, state, state)
    
        # Test adaptation to different input patterns
        # Case 1: Similar input to current state
        similar_input = state + random.normal(key, state.shape) * 0.1
        _, metrics1 = state_manager.apply(
            variables,
            state,
            similar_input,
            threshold=0.5,
            deterministic=True
        )
    
        # Case 2: Very different input
        different_input = random.normal(key, state.shape)
        _, metrics2 = state_manager.apply(
            variables,
            state,
            different_input,
            threshold=0.5,
            deterministic=True
        )
    
        # Memory gate should be more open (lower values) for different inputs
>       assert jnp.mean(metrics1['memory_gate']) > jnp.mean(metrics2['memory_gate'])
E       assert Array(0.50011224, dtype=float32) > Array(0.50012636, dtype=float32)
E        +  where Array(0.50011224, dtype=float32) = <function mean at 0x776c9e670f70>(Array([[0.5569959 , 0.58251977, 0.4551106 , 0.41000992, 0.5886512 ,\n        0.5248869 , 0.56394213, 0.42929018, 0.5237....39520618, 0.44670913, 0.47268876, 0.4021517 ,\n        0.57144237, 0.53708297, 0.579201  , 0.44888377]], dtype=float32))
E        +    where <function mean at 0x776c9e670f70> = jnp.mean
E        +  and   Array(0.50012636, dtype=float32) = <function mean at 0x776c9e670f70>(Array([[0.55393577, 0.57986414, 0.4609477 , 0.4116633 , 0.5870904 ,\n        0.5233553 , 0.5625932 , 0.43273216, 0.5196....39790606, 0.45207238, 0.47798336, 0.4020855 ,\n        0.56832683, 0.5379167 , 0.5762647 , 0.45469669]], dtype=float32))
E        +    where <function mean at 0x776c9e670f70> = jnp.mean

tests/unit/integration/test_state_management.py:125: AssertionError
____________ TestInformationIntegration.test_phi_metric_computation ____________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_integration.TestInformationIntegration object at 0x776c9d4996c0>
key = Array([0, 0], dtype=uint32)
integration_module = InformationIntegration(
    # attributes
    hidden_dim = 64
    num_modules = 4
    dropout_rate = 0.1
)

    def test_phi_metric_computation(self, key, integration_module):
        # Test dimensions
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        # Create sample inputs
        inputs = random.normal(key, (batch_size, num_modules, input_dim))
    
        # Initialize parameters
>       variables = integration_module.init(key, inputs)

tests/unit/memory/test_integration.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:154: in __call__
    output = y + x
../../.local/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573: in deferring_binary_op
    return binary_op(*args)
../../.local/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:179: in __call__
    return call(*args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = Traced<ShapedArray(float32[2,4,64])>with<DynamicJaxprTrace>
y = Traced<ShapedArray(float32[2,4,32])>with<DynamicJaxprTrace>

    @binary_ufunc(identity=0, reduce=reductions.sum, accumulate=reductions.cumsum, at=_add_at)
    def add(x: ArrayLike, y: ArrayLike, /) -> Array:
      """Add two arrays element-wise.
    
      JAX implementation of :obj:`numpy.add`. This is a universal function,
      and supports the additional APIs described at :class:`jax.numpy.ufunc`.
      This function provides the implementation of the ``+`` operator for
      JAX arrays.
    
      Args:
        x, y: arrays to add. Must be broadcastable to a common shape.
    
      Returns:
        Array containing the result of the element-wise addition.
    
      Examples:
        Calling ``add`` explicitly:
    
        >>> x = jnp.arange(4)
        >>> jnp.add(x, 10)
        Array([10, 11, 12, 13], dtype=int32)
    
        Calling ``add`` via the ``+`` operator:
    
        >>> x + 10
        Array([10, 11, 12, 13], dtype=int32)
      """
      x, y = promote_args("add", x, y)
>     return lax.add(x, y) if x.dtype != bool else lax.bitwise_or(x, y)
E     TypeError: add got incompatible shapes for broadcasting: (2, 4, 64), (2, 4, 32).

../../.local/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:1215: TypeError
_______________ TestInformationIntegration.test_information_flow _______________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_integration.TestInformationIntegration object at 0x776c9d4998a0>
key = Array([0, 0], dtype=uint32)
integration_module = InformationIntegration(
    # attributes
    hidden_dim = 64
    num_modules = 4
    dropout_rate = 0.1
)

    def test_information_flow(self, key, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        inputs = random.normal(key, (batch_size, num_modules, input_dim))
>       variables = integration_module.init(key, inputs)

tests/unit/memory/test_integration.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:154: in __call__
    output = y + x
../../.local/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573: in deferring_binary_op
    return binary_op(*args)
../../.local/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:179: in __call__
    return call(*args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = Traced<ShapedArray(float32[2,4,64])>with<DynamicJaxprTrace>
y = Traced<ShapedArray(float32[2,4,32])>with<DynamicJaxprTrace>

    @binary_ufunc(identity=0, reduce=reductions.sum, accumulate=reductions.cumsum, at=_add_at)
    def add(x: ArrayLike, y: ArrayLike, /) -> Array:
      """Add two arrays element-wise.
    
      JAX implementation of :obj:`numpy.add`. This is a universal function,
      and supports the additional APIs described at :class:`jax.numpy.ufunc`.
      This function provides the implementation of the ``+`` operator for
      JAX arrays.
    
      Args:
        x, y: arrays to add. Must be broadcastable to a common shape.
    
      Returns:
        Array containing the result of the element-wise addition.
    
      Examples:
        Calling ``add`` explicitly:
    
        >>> x = jnp.arange(4)
        >>> jnp.add(x, 10)
        Array([10, 11, 12, 13], dtype=int32)
    
        Calling ``add`` via the ``+`` operator:
    
        >>> x + 10
        Array([10, 11, 12, 13], dtype=int32)
      """
      x, y = promote_args("add", x, y)
>     return lax.add(x, y) if x.dtype != bool else lax.bitwise_or(x, y)
E     TypeError: add got incompatible shapes for broadcasting: (2, 4, 64), (2, 4, 32).

../../.local/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:1215: TypeError
_____________ TestInformationIntegration.test_entropy_calculations _____________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_integration.TestInformationIntegration object at 0x776c9d499a80>
key = Array([0, 0], dtype=uint32)
integration_module = InformationIntegration(
    # attributes
    hidden_dim = 64
    num_modules = 4
    dropout_rate = 0.1
)

    def test_entropy_calculations(self, key, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        # Test with different input distributions
        # Uniform distribution
        uniform_input = jnp.ones((batch_size, num_modules, input_dim))
>       variables = integration_module.init(key, uniform_input)

tests/unit/memory/test_integration.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:154: in __call__
    output = y + x
../../.local/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573: in deferring_binary_op
    return binary_op(*args)
../../.local/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:179: in __call__
    return call(*args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = Traced<ShapedArray(float32[2,4,64])>with<DynamicJaxprTrace>
y = Traced<ShapedArray(float32[2,4,32])>with<DynamicJaxprTrace>

    @binary_ufunc(identity=0, reduce=reductions.sum, accumulate=reductions.cumsum, at=_add_at)
    def add(x: ArrayLike, y: ArrayLike, /) -> Array:
      """Add two arrays element-wise.
    
      JAX implementation of :obj:`numpy.add`. This is a universal function,
      and supports the additional APIs described at :class:`jax.numpy.ufunc`.
      This function provides the implementation of the ``+`` operator for
      JAX arrays.
    
      Args:
        x, y: arrays to add. Must be broadcastable to a common shape.
    
      Returns:
        Array containing the result of the element-wise addition.
    
      Examples:
        Calling ``add`` explicitly:
    
        >>> x = jnp.arange(4)
        >>> jnp.add(x, 10)
        Array([10, 11, 12, 13], dtype=int32)
    
        Calling ``add`` via the ``+`` operator:
    
        >>> x + 10
        Array([10, 11, 12, 13], dtype=int32)
      """
      x, y = promote_args("add", x, y)
>     return lax.add(x, y) if x.dtype != bool else lax.bitwise_or(x, y)
E     TypeError: add got incompatible shapes for broadcasting: (2, 4, 64), (2, 4, 32).

../../.local/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:1215: TypeError
______________ TestInformationIntegration.test_memory_integration ______________

self = <test_integration.TestInformationIntegration object at 0x776c9d499c30>

    def test_memory_integration(self):
        batch_size = 2
        hidden_dim = 64
        inputs = jnp.ones((batch_size, hidden_dim))  # Example input
        initial_state = jnp.zeros((batch_size, hidden_dim))  # Ensure correct shape
>       outputs, final_state = memory_module(inputs, initial_state=initial_state)
E       NameError: name 'memory_module' is not defined

tests/unit/memory/test_integration.py:149: NameError
__________________ TestWorkingMemory.test_sequence_processing __________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory.TestWorkingMemory object at 0x776c9d49a8f0>
key = Array([0, 0], dtype=uint32)
memory_module = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)

    def test_sequence_processing(self, key, memory_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
        hidden_dim = 64
    
        # Create sample sequence
        inputs = random.normal(key, (batch_size, seq_length, input_dim))
    
        # Initialize parameters
>       variables = memory_module.init(key, inputs)

tests/unit/memory/test_memory.py:95: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 64
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 64, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
___________________ TestWorkingMemory.test_memory_retention ____________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory.TestWorkingMemory object at 0x776c9d49aad0>
key = Array([0, 0], dtype=uint32)
memory_module = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)

    def test_memory_retention(self, key, memory_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        inputs = random.normal(key, (batch_size, seq_length, input_dim))
>       variables = memory_module.init(key, inputs)

tests/unit/memory/test_memory.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 64
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 64, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
_________________ TestMemoryComponents.test_gru_state_updates __________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory_components.TestMemoryComponents object at 0x776c9d49b7c0>
working_memory = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)
key = Array([ 0, 42], dtype=uint32), batch_size = 2, seq_length = 8
hidden_dim = 64

    def test_gru_state_updates(self, working_memory, key, batch_size, seq_length, hidden_dim):
        """Test GRU cell state updates."""
        inputs = self.create_inputs(key, batch_size, seq_length, hidden_dim)
        initial_state = jnp.zeros((batch_size, hidden_dim))
    
        # Initialize and run forward pass
>       variables = working_memory.init(
            key, inputs, initial_state=initial_state, deterministic=True
        )

tests/unit/memory/test_memory_components.py:36: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 64
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 64, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
_____________ TestMemoryComponents.test_memory_sequence_processing _____________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory_components.TestMemoryComponents object at 0x776c9d49b9d0>
working_memory = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)
key = Array([ 0, 42], dtype=uint32), batch_size = 2, seq_length = 8
hidden_dim = 64

    def test_memory_sequence_processing(self, working_memory, key, batch_size, seq_length, hidden_dim):
        """Test working memory sequence processing."""
        # Test with different sequence lengths
        for test_length in [4, 8, 16]:
            inputs = self.create_inputs(key, batch_size, test_length, hidden_dim)
            initial_state = jnp.zeros((batch_size, hidden_dim))
    
>           variables = working_memory.init(
                key, inputs, initial_state=initial_state, deterministic=True
            )

tests/unit/memory/test_memory_components.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 64
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 64, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
________________ TestMemoryComponents.test_context_aware_gating ________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory_components.TestMemoryComponents object at 0x776c9d49bbe0>
working_memory = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)
key = Array([ 0, 42], dtype=uint32), batch_size = 2, seq_length = 8
hidden_dim = 64

    def test_context_aware_gating(self, working_memory, key, batch_size, seq_length, hidden_dim):
        """Test context-aware gating mechanisms."""
        # Create two different input sequences with controlled differences
        base_inputs = self.create_inputs(key, batch_size, seq_length, hidden_dim)
    
        # Create similar and different inputs
        similar_inputs = base_inputs + jax.random.normal(key, base_inputs.shape) * 0.1
        different_inputs = jax.random.normal(
            jax.random.fold_in(key, 1),
            base_inputs.shape
        )
    
        initial_state = jnp.zeros((batch_size, hidden_dim))
>       variables = working_memory.init(
            key, base_inputs, initial_state=initial_state, deterministic=True
        )

tests/unit/memory/test_memory_components.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 64
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 64, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
______________ TestMemoryComponents.test_information_integration _______________

self = <test_memory_components.TestMemoryComponents object at 0x776c9d49bdf0>
info_integration = InformationIntegration(
    # attributes
    hidden_dim = 64
    num_modules = 4
    dropout_rate = 0.1
)
key = Array([ 0, 42], dtype=uint32), batch_size = 2, seq_length = 8
hidden_dim = 64

    def test_information_integration(self, info_integration, key, batch_size, seq_length, hidden_dim):
        """Test information integration computation."""
        # Create inputs with proper shape for information integration
        inputs = jnp.stack([
            self.create_inputs(jax.random.fold_in(key, i), batch_size, seq_length, hidden_dim)
            for i in range(info_integration.num_modules)
        ], axis=1)  # Shape: [batch, num_modules, seq_length, hidden_dim]
    
        # Initialize and run forward pass
        variables = info_integration.init(key, inputs, deterministic=True)
        output, phi = info_integration.apply(variables, inputs, deterministic=True)
    
        # Verify shapes
        expected_output_shape = (batch_size, info_integration.num_modules, seq_length, hidden_dim)
        self.assert_output_shape(output, expected_output_shape)
    
        # Phi should be a scalar per batch element
>       assert phi.shape == (batch_size,)
E       assert (2, 8) == (2,)
E         
E         Left contains one more item: 8
E         Use -v to get more diff

tests/unit/memory/test_memory_components.py:117: AssertionError
__________________ TestMemoryComponents.test_memory_retention __________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory_components.TestMemoryComponents object at 0x776c9d4c0040>
working_memory = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)
key = Array([ 0, 42], dtype=uint32), batch_size = 2, seq_length = 8
hidden_dim = 64

    def test_memory_retention(self, working_memory, key, batch_size, seq_length, hidden_dim):
        """Test memory retention over sequences."""
        # Create a sequence with a distinctive pattern
        pattern = jnp.ones((batch_size, 1, hidden_dim))
        inputs = jnp.concatenate([
            pattern,
            self.create_inputs(key, batch_size, seq_length-2, hidden_dim),
            pattern
        ], axis=1)
    
        initial_state = jnp.zeros((batch_size, hidden_dim))
    
>       variables = working_memory.init(
            key, inputs, initial_state=initial_state, deterministic=True
        )

tests/unit/memory/test_memory_components.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:91: in __call__
    rnn_cell = nn.LSTMCell(features=self.hidden_dim)
../../.local/lib/python3.10/site-packages/flax/linen/kw_only_dataclasses.py:235: in init_wrapper
    dataclass_init(self, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(
    # attributes
    features = 64
    gate_fn = sigmoid
    activation_fn = tanh
    kernel_init = init
    recurrent_kernel_init = init
    bias_init = zeros
    dtype = None
    param_dtype = float32
    carry_init = zeros
)
features = 64, gate_fn = <PjitFunction of <function sigmoid at 0x776c9df263b0>>
activation_fn = <PjitFunction of <function tanh at 0x776c9e6f0e50>>
kernel_init = <function variance_scaling.<locals>.init at 0x776c9deac9d0>
recurrent_kernel_init = <function orthogonal.<locals>.init at 0x776c9dda7eb0>
bias_init = <function zeros at 0x776c9e065480>, dtype = None
param_dtype = <class 'jax.numpy.float32'>
carry_init = <function zeros at 0x776c9e065480>
parent = <flax.linen.module._Sentinel object at 0x776c9de7bb20>, name = None

>   ???
E   flax.errors.AssignSubModuleError: Submodule LSTMCell must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)

<string>:14: AssignSubModuleError
=========================== short test summary info ============================
FAILED tests/benchmarks/test_arc_reasoning.py::TestARCReasoning::test_pattern_recognition
FAILED tests/benchmarks/test_arc_reasoning.py::TestARCReasoning::test_abstraction_capability
FAILED tests/benchmarks/test_arc_reasoning.py::TestARCReasoning::test_conscious_adaptation
FAILED tests/benchmarks/test_bigbench_reasoning.py::TestBigBenchReasoning::test_reasoning_capabilities
FAILED tests/benchmarks/test_bigbench_reasoning.py::TestBigBenchReasoning::test_meta_learning
FAILED tests/benchmarks/test_bigbench_reasoning.py::TestBigBenchReasoning::test_consciousness_emergence
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_forward_pass
FAILED tests/unit/attention/test_attention.py::TestConsciousnessAttention::test_attention_output_shape
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cross_modal_attention
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_modality_specific_processing
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_integration_stability
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cognitive_integration
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_rl_optimization
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_phi_metric_computation
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_information_flow
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_entropy_calculations
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_memory_integration
FAILED tests/unit/memory/test_memory.py::TestWorkingMemory::test_sequence_processing
FAILED tests/unit/memory/test_memory.py::TestWorkingMemory::test_memory_retention
FAILED tests/unit/memory/test_memory_components.py::TestMemoryComponents::test_gru_state_updates
FAILED tests/unit/memory/test_memory_components.py::TestMemoryComponents::test_memory_sequence_processing
FAILED tests/unit/memory/test_memory_components.py::TestMemoryComponents::test_context_aware_gating
FAILED tests/unit/memory/test_memory_components.py::TestMemoryComponents::test_information_integration
FAILED tests/unit/memory/test_memory_components.py::TestMemoryComponents::test_memory_retention
======================== 25 failed, 23 passed in 14.73s ========================
