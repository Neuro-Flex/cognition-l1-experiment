============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.4, pluggy-1.5.0
rootdir: /home/kasinadhsarma/experiment/cognition-l1-experiment
configfile: pytest.ini
testpaths: tests
plugins: cov-6.0.0, anyio-4.7.0
collected 51 items

tests/benchmarks/test_arc_reasoning.py FFF                               [  5%]
tests/benchmarks/test_bigbench_reasoning.py FFF                          [ 11%]
tests/test_consciousness.py .F.FFF                                       [ 23%]
tests/test_environment.py .....                                          [ 33%]
tests/unit/attention/test_attention.py ....                              [ 41%]
tests/unit/attention/test_attention_mechanisms.py ....                   [ 49%]
tests/unit/integration/test_cognitive_integration.py FF.F                [ 56%]
tests/unit/integration/test_state_management.py ..F.                     [ 64%]
tests/unit/memory/test_integration.py FF.F                               [ 72%]
tests/unit/memory/test_memory.py ..FF                                    [ 80%]
tests/unit/memory/test_memory_components.py .FFFF                        [ 90%]
tests/unit/state/test_consciousness_state_management.py .....            [100%]

=================================== FAILURES ===================================
__________________ TestARCReasoning.test_pattern_recognition ___________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_arc_reasoning.TestARCReasoning object at 0x72f182c32f80>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_pattern_recognition(self, key, consciousness_model):
        inputs, expected = self.load_arc_sample()
        batch_size = inputs['visual'].shape[0]
    
        # Initialize model state
        model_inputs = {
            'visual': inputs['visual'],
            'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))
        }
    
        # Ensure input_shape is a tuple
        input_shape = (batch_size,)
        # Initialize model
>       variables = consciousness_model.init(key, model_inputs)

tests/benchmarks/test_arc_reasoning.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:105: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([2420929151, 3698740751], dtype=uint32), rng = (1, 512)
input_shape = 512

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
_________________ TestARCReasoning.test_abstraction_capability _________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_arc_reasoning.TestARCReasoning object at 0x72f182c30ac0>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_abstraction_capability(self, key, consciousness_model):
        inputs, _ = self.load_arc_sample()
        batch_size = inputs['visual'].shape[0]
    
        # Create transformed versions
        variations = {
            'original': inputs['visual'],
            'rotated': jnp.rot90(inputs['visual'][:, :, :, 0], k=1)[:, :, None],
            'scaled': inputs['visual'] * 2.0
        }
    
        try:
            # Ensure input_shape is a tuple
            input_shape = (batch_size,)
>           variables = consciousness_model.init(
                key,
                {'visual': variations['original'],
                 'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))}
            )

tests/benchmarks/test_arc_reasoning.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:105: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([2420929151, 3698740751], dtype=uint32), rng = (1, 512)
input_shape = 512

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError

During handling of the above exception, another exception occurred:

self = <tests.benchmarks.test_arc_reasoning.TestARCReasoning object at 0x72f182c30ac0>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_abstraction_capability(self, key, consciousness_model):
        inputs, _ = self.load_arc_sample()
        batch_size = inputs['visual'].shape[0]
    
        # Create transformed versions
        variations = {
            'original': inputs['visual'],
            'rotated': jnp.rot90(inputs['visual'][:, :, :, 0], k=1)[:, :, None],
            'scaled': inputs['visual'] * 2.0
        }
    
        try:
            # Ensure input_shape is a tuple
            input_shape = (batch_size,)
            variables = consciousness_model.init(
                key,
                {'visual': variations['original'],
                 'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))}
            )
    
            states = {}
            for name, visual_input in variations.items():
                output, metrics = consciousness_model.apply(
                    variables,
                    {'visual': visual_input,
                     'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))},
                    deterministic=True
                )
                states[name] = output
    
            # Test representation similarity
            def cosine_similarity(x, y):
                return jnp.sum(x * y) / (jnp.linalg.norm(x) * jnp.linalg.norm(y))
    
            orig_rot_sim = cosine_similarity(
                states['original'].ravel(),
                states['rotated'].ravel()
            )
            orig_scaled_sim = cosine_similarity(
                states['original'].ravel(),
                states['scaled'].ravel()
            )
    
            # Transformed versions should maintain similar representations
            assert orig_rot_sim > 0.5
            assert orig_scaled_sim > 0.7
    
        except Exception as e:
>           pytest.fail(f"Abstraction capability test failed: {str(e)}")
E           Failed: Abstraction capability test failed: 'int' object is not subscriptable

tests/benchmarks/test_arc_reasoning.py:136: Failed
__________________ TestARCReasoning.test_conscious_adaptation __________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_arc_reasoning.TestARCReasoning object at 0x72f182c31720>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_conscious_adaptation(self, key, consciousness_model):
        inputs, _ = self.load_arc_sample()
        batch_size = inputs['visual'].shape[0]
    
        try:
            # Create simple and complex patterns
            simple_input = {
                'visual': inputs['visual'],
                'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))
            }
    
            # More complex pattern (doubled size)
            complex_visual = jnp.tile(inputs['visual'], (1, 2, 2, 1))
            complex_input = {
                'visual': complex_visual,
                'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))
            }
    
            # Ensure input_shape is a tuple
            input_shape = (batch_size,)
>           variables = consciousness_model.init(key, simple_input)

tests/benchmarks/test_arc_reasoning.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:105: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([2420929151, 3698740751], dtype=uint32), rng = (1, 512)
input_shape = 512

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError

During handling of the above exception, another exception occurred:

self = <tests.benchmarks.test_arc_reasoning.TestARCReasoning object at 0x72f182c31720>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_conscious_adaptation(self, key, consciousness_model):
        inputs, _ = self.load_arc_sample()
        batch_size = inputs['visual'].shape[0]
    
        try:
            # Create simple and complex patterns
            simple_input = {
                'visual': inputs['visual'],
                'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))
            }
    
            # More complex pattern (doubled size)
            complex_visual = jnp.tile(inputs['visual'], (1, 2, 2, 1))
            complex_input = {
                'visual': complex_visual,
                'state': jnp.zeros((batch_size, consciousness_model.hidden_dim))
            }
    
            # Ensure input_shape is a tuple
            input_shape = (batch_size,)
            variables = consciousness_model.init(key, simple_input)
    
            # Process both patterns
            _, simple_metrics = consciousness_model.apply(
                variables,
                simple_input,
                deterministic=True
            )
    
            _, complex_metrics = consciousness_model.apply(
                variables,
                complex_input,
                deterministic=True
            )
    
            # Validate complexity adaptation
            assert complex_metrics['phi'] > simple_metrics['phi']
            assert 'attention_weights' in simple_metrics
            assert 'attention_weights' in complex_metrics
    
            # Validate attention maps
            assert 'attention_maps' in simple_metrics
            assert 'attention_maps' in complex_metrics
            for attn_map in simple_metrics['attention_maps'].values():
                assert jnp.allclose(
                    jnp.sum(attn_map, axis=-1),
                    jnp.ones((batch_size, 8, 64))  # (batch, heads, seq_length)
                )
            for attn_map in complex_metrics['attention_maps'].values():
                assert jnp.allclose(
                    jnp.sum(attn_map, axis=-1),
                    jnp.ones((batch_size, 8, 64))  # (batch, heads, seq_length)
                )
    
        except Exception as e:
>           pytest.fail(f"Conscious adaptation test failed: {str(e)}")
E           Failed: Conscious adaptation test failed: 'int' object is not subscriptable

tests/benchmarks/test_arc_reasoning.py:193: Failed
______________ TestBigBenchReasoning.test_reasoning_capabilities _______________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_bigbench_reasoning.TestBigBenchReasoning object at 0x72f182c334c0>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_reasoning_capabilities(self, key, consciousness_model):
        tasks = self.load_sample_tasks()
        input_shape = (1,)
>       variables = consciousness_model.init(key, {'textual': jnp.zeros((1, 1, 512))})

tests/benchmarks/test_bigbench_reasoning.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:105: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([2420929151, 3698740751], dtype=uint32), rng = (1, 512)
input_shape = 512

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
___________________ TestBigBenchReasoning.test_meta_learning ___________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_bigbench_reasoning.TestBigBenchReasoning object at 0x72f182c33a00>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_meta_learning(self, key, consciousness_model):
        """Test model's ability to adapt to new reasoning patterns."""
        # Create sequence of related but progressively complex tasks
        sequence = [
            {'textual': "1, 2, 3, _", 'expected': "4"},
            {'textual': "2, 4, 6, _", 'expected': "8"},
            {'textual': "3, 6, 9, _", 'expected': "12"}
        ]
    
        input_shape = (1,)
>       variables = consciousness_model.init(
            key,
            {'textual': jnp.zeros((1, 1, 512))}
        )

tests/benchmarks/test_bigbench_reasoning.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:105: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([2420929151, 3698740751], dtype=uint32), rng = (1, 512)
input_shape = 512

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
______________ TestBigBenchReasoning.test_consciousness_emergence ______________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.benchmarks.test_bigbench_reasoning.TestBigBenchReasoning object at 0x72f182c332b0>
key = Array([0, 0], dtype=uint32)
consciousness_model = ConsciousnessModel(
    # attributes
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    num_states = 4
    dropout_rate = 0.1
)

    def test_consciousness_emergence(self, key, consciousness_model):
        """
        Test for emergence of consciousness-like behaviors:
        1. Integration of information
        2. Adaptive processing
        3. Self-monitoring
        """
        # Complex multi-step reasoning task
        task_embedding = random.normal(key, (1, 128, 512))
        input_shape = (1,)
>       variables = consciousness_model.init(
            key,
            {'textual': task_embedding}
        )

tests/benchmarks/test_bigbench_reasoning.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:105: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([2420929151, 3698740751], dtype=uint32), rng = (1, 512)
input_shape = 512

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
________________ TestConsciousnessModel.test_model_forward_pass ________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.test_consciousness.TestConsciousnessModel object at 0x72f182c687f0>
model = ConsciousnessModel(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 4
    num_states = 4
    dropout_rate = 0.1
)
sample_input = {'attention': Array([[[-0.02862089,  1.5240539 , -1.0556508 , ..., -0.27188757,
         -0.88195777,  0.11891642],
  ...        [-0.5756394 , -0.20118208, -0.08988765, ...,  0.23238769,
          1.5470275 , -1.2839596 ]]], dtype=float32)}
key = Array([ 0, 42], dtype=uint32), deterministic = True

    def test_model_forward_pass(self, model, sample_input, key, deterministic):
        """Test forward pass through consciousness model."""
        # Initialize model
        input_shape = (sample_input['attention'].shape[0],)
>       variables = model.init(key, sample_input, deterministic=deterministic)

tests/test_consciousness.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:105: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([3819375347, 1502290012], dtype=uint32), rng = (2, 64)
input_shape = 64

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
____________ TestConsciousnessModel.test_model_state_initialization ____________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.test_consciousness.TestConsciousnessModel object at 0x72f182c68c40>
model = ConsciousnessModel(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 4
    num_states = 4
    dropout_rate = 0.1
)
sample_input = {'attention': Array([[[-0.02862089,  1.5240539 , -1.0556508 , ..., -0.27188757,
         -0.88195777,  0.11891642],
  ...        [-0.5756394 , -0.20118208, -0.08988765, ...,  0.23238769,
          1.5470275 , -1.2839596 ]]], dtype=float32)}
key = Array([ 0, 42], dtype=uint32), deterministic = True

    def test_model_state_initialization(self, model, sample_input, key, deterministic):
        """Test initialization of the model state."""
        input_shape = (sample_input['attention'].shape[0],)
>       variables = model.init(key, sample_input, deterministic=deterministic)

tests/test_consciousness.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:105: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([3819375347, 1502290012], dtype=uint32), rng = (2, 64)
input_shape = 64

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
________________ TestConsciousnessModel.test_model_state_update ________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.test_consciousness.TestConsciousnessModel object at 0x72f182c68eb0>
model = ConsciousnessModel(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 4
    num_states = 4
    dropout_rate = 0.1
)
sample_input = {'attention': Array([[[-0.02862089,  1.5240539 , -1.0556508 , ..., -0.27188757,
         -0.88195777,  0.11891642],
  ...        [-0.5756394 , -0.20118208, -0.08988765, ...,  0.23238769,
          1.5470275 , -1.2839596 ]]], dtype=float32)}
key = Array([ 0, 42], dtype=uint32), deterministic = True

    def test_model_state_update(self, model, sample_input, key, deterministic):
        """Test updating the model state."""
        input_shape = (sample_input['attention'].shape[0],)
>       variables = model.init(key, sample_input, deterministic=deterministic)

tests/test_consciousness.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:105: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([3819375347, 1502290012], dtype=uint32), rng = (2, 64)
input_shape = 64

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
_____________ TestConsciousnessModel.test_model_attention_weights ______________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <tests.test_consciousness.TestConsciousnessModel object at 0x72f182c69120>
model = ConsciousnessModel(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 4
    num_states = 4
    dropout_rate = 0.1
)
sample_input = {'attention': Array([[[-0.02862089,  1.5240539 , -1.0556508 , ..., -0.27188757,
         -0.88195777,  0.11891642],
  ...        [-0.5756394 , -0.20118208, -0.08988765, ...,  0.23238769,
          1.5470275 , -1.2839596 ]]], dtype=float32)}
key = Array([ 0, 42], dtype=uint32), deterministic = True

    def test_model_attention_weights(self, model, sample_input, key, deterministic):
        """Test attention weights in the model."""
        input_shape = (sample_input['attention'].shape[0],)
>       variables = model.init(key, sample_input, deterministic=deterministic)

tests/test_consciousness.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_model.py:105: in __call__
    memory_output, memory_state = self.working_memory(
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([3819375347, 1502290012], dtype=uint32), rng = (2, 64)
input_shape = 64

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
__________ TestCognitiveProcessIntegration.test_cross_modal_attention __________

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x72f1751a10f0>
key = Array([0, 0], dtype=uint32)
integration_module = CognitiveProcessIntegration(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 3
    dropout_rate = 0.1
)

    def test_cross_modal_attention(self, key, integration_module):
        # Test dimensions
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Create multi-modal inputs
        inputs = {
            'visual': random.normal(key, (batch_size, seq_length, input_dim)),
            'textual': random.normal(
                random.PRNGKey(1),
                (batch_size, seq_length, input_dim)
            ),
            'numerical': random.normal(
                random.PRNGKey(2),
                (batch_size, seq_length, input_dim)
            )
        }
    
        # Initialize parameters
        input_shape = (batch_size,)
        variables = integration_module.init(key, inputs)
    
        # Process through integration
        consciousness_state, attention_maps = integration_module.apply(
            variables,
            inputs,
            deterministic=True
        )
    
        # Test output shapes
        assert consciousness_state.shape == (batch_size, seq_length, 64)
    
        # Test attention maps
        for source in inputs.keys():
            for target in inputs.keys():
                if source != target:
                    map_key = f"{target}-{source}"
                    assert map_key in attention_maps
                    attention_map = attention_maps[map_key]
                    # Check attention map properties
>                   assert attention_map.shape[-2:] == (seq_length, seq_length)
E                   assert (8, 32) == (8, 8)
E                     
E                     At index 1 diff: 32 != 8
E                     Use -v to get more diff

tests/unit/integration/test_cognitive_integration.py:68: AssertionError
______ TestCognitiveProcessIntegration.test_modality_specific_processing _______
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x72f1751a3640>
key = Array([0, 0], dtype=uint32)
integration_module = CognitiveProcessIntegration(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 3
    dropout_rate = 0.1
)

    def test_modality_specific_processing(self, key, integration_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Test with single modality
        single_input = {
            'visual': random.normal(key, (batch_size, seq_length, input_dim))
        }
        input_shape = (batch_size,)
        variables = integration_module.init(key, single_input)
    
        consciousness_state1, _ = integration_module.apply(
            variables,
            single_input,
            deterministic=True
        )
    
        # Test with multiple modalities
        multi_input = {
            'visual': single_input['visual'],
            'textual': random.normal(key, (batch_size, seq_length, input_dim))
        }
    
>       consciousness_state2, _ = integration_module.apply(
            variables,
            multi_input,
            deterministic=True
        )

tests/unit/integration/test_cognitive_integration.py:99: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/consciousness_state.py:21: in __call__
    x = nn.LayerNorm()(x)
../../.local/lib/python3.10/site-packages/flax/linen/normalization.py:518: in __call__
    return _normalize(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

mdl = LayerNorm(
    # attributes
    epsilon = 1e-06
    dtype = None
    param_dtype = float32
    use_bias = True
    use...-1
    axis_name = None
    axis_index_groups = None
    use_fast_variance = True
    force_float32_reductions = True
)
x = Array([[[-2.0545669 ,  0.33981755,  0.2619878 , -0.9536857 ,
          1.1861376 ,  1.2227523 ,  0.1118008 , -0.327318...86593276,  0.17548549, -2.5559947 ,
          0.0899727 , -1.434366  ,  0.5375884 ,  1.2183367 ]]],      dtype=float32)
mean = Array([[[ 0.316849  ],
        [ 0.15681918],
        [-0.09266028],
        [-0.09894891],
        [-0.3263538 ],
   ...16475785],
        [-0.13104448],
        [ 0.06134801],
        [-0.21195135],
        [-0.32997662]]], dtype=float32)
var = Array([[[0.85883766],
        [0.66789114],
        [0.95650977],
        [1.002326  ],
        [0.5636929 ],
        ... [0.5845102 ],
        [1.0722553 ],
        [1.0433921 ],
        [0.83152187],
        [1.3724558 ]]], dtype=float32)
reduction_axes = (2,), feature_axes = (2,), dtype = None
param_dtype = <class 'jax.numpy.float32'>, epsilon = 1e-06, use_bias = True
use_scale = True, bias_init = <function zeros at 0x72f183861480>
scale_init = <function ones at 0x72f1838fca60>, force_float32_reductions = True

    def _normalize(
      mdl: Module,
      x: Array,
      mean: Array,
      var: Array,
      reduction_axes: Axes,
      feature_axes: Axes,
      dtype: Dtype | None,
      param_dtype: Dtype,
      epsilon: float,
      use_bias: bool,
      use_scale: bool,
      bias_init: Initializer,
      scale_init: Initializer,
      force_float32_reductions: bool = True
    ):
      """Normalizes the input of a normalization layer and optionally applies a learned scale and bias.
    
      Arguments:
        mdl: Module to apply the normalization in (normalization params will reside
          in this module).
        x: The input.
        mean: Mean to use for normalization.
        var: Variance to use for normalization.
        reduction_axes: The axes in ``x`` to reduce.
        feature_axes: Axes containing features. A separate bias and scale is learned
          for each specified feature.
        dtype: The dtype of the result (default: infer from input and params).
        param_dtype: The dtype of the parameters.
        epsilon: Normalization epsilon.
        use_bias: If true, add a bias term to the output.
        use_scale: If true, scale the output.
        bias_init: Initialization function for the bias term.
        scale_init: Initialization function for the scaling function.
        force_float32_reductions: If false, the scale and bias parameters use the
          param_dtype. Otherwise, they will have at least float32 precision due to
          the mean and var being promoted to float32.
    
      Returns:
        The normalized input.
      """
      reduction_axes = _canonicalize_axes(x.ndim, reduction_axes)
      feature_axes = _canonicalize_axes(x.ndim, feature_axes)
      feature_shape = [1] * x.ndim
      reduced_feature_shape = []
      for ax in feature_axes:
        feature_shape[ax] = x.shape[ax]
        reduced_feature_shape.append(x.shape[ax])
    
      mean = jnp.expand_dims(mean, reduction_axes)
      var = jnp.expand_dims(var, reduction_axes)
      y = x - mean
      mul = lax.rsqrt(var + epsilon)
      args = [x]
      if use_scale:
>       scale = mdl.param(
          'scale', scale_init, reduced_feature_shape, param_dtype
        ).reshape(feature_shape)
E       flax.errors.ScopeParamNotFoundError: Could not find parameter named "scale" in scope "/LayerNorm_1". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamNotFoundError)

../../.local/lib/python3.10/site-packages/flax/linen/normalization.py:204: ScopeParamNotFoundError
__________ TestCognitiveProcessIntegration.test_cognitive_integration __________

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x72f1751a3a00>
key = Array([0, 0], dtype=uint32)
integration_module = CognitiveProcessIntegration(
    # attributes
    hidden_dim = 64
    num_heads = 4
    num_layers = 3
    dropout_rate = 0.1
)

    def test_cognitive_integration(self, key, integration_module):
        # Test dimensions
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Create multi-modal inputs
        inputs = {
            'visual': random.normal(random.PRNGKey(0), (batch_size, seq_length, input_dim)),
            'textual': random.normal(random.PRNGKey(1), (batch_size, seq_length, input_dim)),
            'numerical': random.normal(random.PRNGKey(2), (batch_size, seq_length, input_dim))
        }
    
        # Initialize parameters
        input_shape = (batch_size,)
        variables = integration_module.init(random.PRNGKey(0), inputs)
    
        # Process through integration
        consciousness_state, attention_maps = integration_module.apply(
            variables,
            inputs,
            deterministic=True
        )
    
        # Adjust assertions as needed
        assert consciousness_state.shape == (batch_size, seq_length, 64)
        for source in inputs.keys():
            for target in inputs.keys():
                if source != target:
                    map_key = f"{target}-{source}"
                    assert map_key in attention_maps
                    attention_map = attention_maps[map_key]
>                   assert attention_map.shape[-2:] == (seq_length, seq_length)
E                   assert (8, 32) == (8, 8)
E                     
E                     At index 1 diff: 32 != 8
E                     Use -v to get more diff

tests/unit/integration/test_cognitive_integration.py:183: AssertionError
______________ TestConsciousnessStateManager.test_adaptive_gating ______________

self = <test_state_management.TestConsciousnessStateManager object at 0x72f1751d0880>
key = Array([0, 0], dtype=uint32)
state_manager = ConsciousnessStateManager(
    # attributes
    hidden_dim = 64
    num_states = 4
    dropout_rate = 0.1
)

    def test_adaptive_gating(self, key, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = random.normal(key, (batch_size, hidden_dim))
        input_shape = (batch_size,)
        variables = state_manager.init(key, state, state)
    
        # Test adaptation to different input patterns
        # Case 1: Similar input to current state
        similar_input = state + random.normal(key, state.shape) * 0.1
        _, metrics1 = state_manager.apply(
            variables,
            state,
            similar_input,
            threshold=0.5,
            deterministic=True
        )
    
        # Case 2: Very different input
        different_input = random.normal(key, state.shape)
        _, metrics2 = state_manager.apply(
            variables,
            state,
            different_input,
            threshold=0.5,
            deterministic=True
        )
    
        # Memory gate should be more open (lower values) for different inputs
>       assert jnp.mean(metrics1['memory_gate']) > jnp.mean(metrics2['memory_gate'])
E       assert Array(0.50011224, dtype=float32) > Array(0.50012636, dtype=float32)
E        +  where Array(0.50011224, dtype=float32) = <function mean at 0x72f183e70f70>(Array([[0.5569959 , 0.58251977, 0.4551106 , 0.41000992, 0.5886512 ,\n        0.5248869 , 0.56394213, 0.42929018, 0.5237....39520618, 0.44670913, 0.47268876, 0.4021517 ,\n        0.57144237, 0.53708297, 0.579201  , 0.44888377]], dtype=float32))
E        +    where <function mean at 0x72f183e70f70> = jnp.mean
E        +  and   Array(0.50012636, dtype=float32) = <function mean at 0x72f183e70f70>(Array([[0.55393577, 0.57986414, 0.4609477 , 0.4116633 , 0.5870904 ,\n        0.5233553 , 0.5625932 , 0.43273216, 0.5196....39790606, 0.45207238, 0.47798336, 0.4020855 ,\n        0.56832683, 0.5379167 , 0.5762647 , 0.45469669]], dtype=float32))
E        +    where <function mean at 0x72f183e70f70> = jnp.mean

tests/unit/integration/test_state_management.py:126: AssertionError
____________ TestInformationIntegration.test_phi_metric_computation ____________

self = <test_integration.TestInformationIntegration object at 0x72f1751d1750>
key = Array([0, 0], dtype=uint32)
integration_module = InformationIntegration(
    # attributes
    hidden_dim = 64
    num_modules = 4
    dropout_rate = 0.1
)

    def test_phi_metric_computation(self, key, integration_module):
        # Test dimensions
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        # Create sample inputs
        inputs = random.normal(key, (batch_size, num_modules, input_dim))
    
        # Initialize parameters
        input_shape = (batch_size,)
        variables = integration_module.init(key, inputs)
    
        # Process through integration
        output, phi = integration_module.apply(
            variables,
            inputs,
            deterministic=True
        )
    
        # Test output shapes
>       assert output.shape == inputs.shape
E       assert (2, 4, 64) == (2, 4, 32)
E         
E         At index 2 diff: 64 != 32
E         Use -v to get more diff

tests/unit/memory/test_integration.py:46: AssertionError
_______________ TestInformationIntegration.test_information_flow _______________

self = <test_integration.TestInformationIntegration object at 0x72f1751d1930>
key = Array([0, 0], dtype=uint32)
integration_module = InformationIntegration(
    # attributes
    hidden_dim = 64
    num_modules = 4
    dropout_rate = 0.1
)

    def test_information_flow(self, key, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        inputs = jnp.zeros((2, 4, 64), dtype=jnp.float32)  # ensure shape matches the model
        input_shape = (batch_size,)
        variables = integration_module.init(key, inputs)
    
        # Test with and without dropout
        output1, _ = integration_module.apply(
            variables,
            inputs,
            deterministic=False,
            rngs={'dropout': random.PRNGKey(1)}
        )
    
        output2, _ = integration_module.apply(
            variables,
            inputs,
            deterministic=True
        )
    
        # Test residual connection properties
        # Output should maintain some similarity with input
        input_output_correlation = jnp.mean(jnp.abs(
            jnp.corrcoef(
                inputs.reshape(-1, input_dim),
                output2.reshape(-1, input_dim)
            )
        ))
>       assert input_output_correlation > 0.1
E       assert Array(nan, dtype=float32) > 0.1

tests/unit/memory/test_integration.py:106: AssertionError
______________ TestInformationIntegration.test_memory_integration ______________

self = <test_integration.TestInformationIntegration object at 0x72f1751d1cf0>
key = Array([0, 0], dtype=uint32)
integration_module = InformationIntegration(
    # attributes
    hidden_dim = 64
    num_modules = 4
    dropout_rate = 0.1
)

    def test_memory_integration(self, key, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        inputs = random.normal(key, (batch_size, num_modules, input_dim))
        input_shape = (batch_size,)
        variables = integration_module.init(key, inputs)
    
        # Process through integration
        output, phi = integration_module.apply(
            variables,
            inputs,
            deterministic=True
        )
    
        # Test output shapes
>       assert output.shape == inputs.shape
E       assert (2, 4, 64) == (2, 4, 32)
E         
E         At index 2 diff: 64 != 32
E         Use -v to get more diff

tests/unit/memory/test_integration.py:163: AssertionError
__________________ TestWorkingMemory.test_sequence_processing __________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory.TestWorkingMemory object at 0x72f1751d2770>
key = Array([0, 0], dtype=uint32)
memory_module = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)

    def test_sequence_processing(self, key, memory_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
        hidden_dim = 64
    
        # Create sample sequence
        inputs = random.normal(key, (batch_size, seq_length, input_dim))
    
        # Initialize parameters
        input_shape = (batch_size,)
>       variables = memory_module.init(key, inputs, deterministic=True)

tests/unit/memory/test_memory.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([1428664606, 3351135085], dtype=uint32), rng = (2, 64)
input_shape = 64

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
___________________ TestWorkingMemory.test_memory_retention ____________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory.TestWorkingMemory object at 0x72f1751d2950>
key = Array([0, 0], dtype=uint32)
memory_module = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)

    def test_memory_retention(self, key, memory_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        inputs = random.normal(key, (batch_size, seq_length, input_dim))
        input_shape = (batch_size,)
>       variables = memory_module.init(key, inputs)

tests/unit/memory/test_memory.py:132: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([1428664606, 3351135085], dtype=uint32), rng = (2, 64)
input_shape = 64

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
_____________ TestMemoryComponents.test_memory_sequence_processing _____________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory_components.TestMemoryComponents object at 0x72f1751f0070>
working_memory = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)
key = Array([0, 0], dtype=uint32), batch_size = 2, seq_length = 8
hidden_dim = 64

    def test_memory_sequence_processing(self, working_memory, key, batch_size, seq_length, hidden_dim):
        """Test working memory sequence processing."""
        # Test with different sequence lengths
        for test_length in [4, 8, 16]:
            inputs = self.create_inputs(key, batch_size, test_length, hidden_dim)
            initial_state = jnp.zeros((batch_size, hidden_dim))
    
            input_shape = (batch_size,)
>           variables = working_memory.init(
                key, inputs, initial_state=initial_state, deterministic=True
            )

tests/unit/memory/test_memory_components.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([1428664606, 3351135085], dtype=uint32), rng = (2, 64)
input_shape = 64

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
________________ TestMemoryComponents.test_context_aware_gating ________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory_components.TestMemoryComponents object at 0x72f1751f0280>
working_memory = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)
key = Array([0, 0], dtype=uint32), batch_size = 2, seq_length = 8
hidden_dim = 64

    def test_context_aware_gating(self, working_memory, key, batch_size, seq_length, hidden_dim):
        """Test context-aware gating mechanisms."""
        # Create two different input sequences with controlled differences
        base_inputs = self.create_inputs(key, batch_size, seq_length, hidden_dim)
    
        # Create similar and different inputs
        similar_inputs = base_inputs + jax.random.normal(key, base_inputs.shape) * 0.1
        different_inputs = jax.random.normal(
            jax.random.fold_in(key, 1),
            base_inputs.shape
        )
    
        initial_state = jnp.zeros((batch_size, hidden_dim))
        input_shape = (batch_size,)
>       variables = working_memory.init(
            key, base_inputs, initial_state=initial_state, deterministic=True
        )

tests/unit/memory/test_memory_components.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([1428664606, 3351135085], dtype=uint32), rng = (2, 64)
input_shape = 64

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
______________ TestMemoryComponents.test_information_integration _______________

self = <test_memory_components.TestMemoryComponents object at 0x72f1751f0490>
info_integration = InformationIntegration(
    # attributes
    hidden_dim = 64
    num_modules = 4
    dropout_rate = 0.1
)
key = Array([0, 0], dtype=uint32), batch_size = 2, seq_length = 8
hidden_dim = 64

    def test_information_integration(self, info_integration, key, batch_size, seq_length, hidden_dim):
        """Test information integration computation."""
        # Create inputs with proper shape for information integration
        inputs = jnp.stack([
            self.create_inputs(jax.random.fold_in(key, i), batch_size, seq_length, hidden_dim)
            for i in range(info_integration.num_modules)
        ], axis=1)  # Shape: [batch, num_modules, seq_length, hidden_dim]
    
        # Initialize and run forward pass
        input_shape = (batch_size,)
        variables = info_integration.init(key, inputs, deterministic=True)
        output, phi = info_integration.apply(variables, inputs, deterministic=True)
    
        # Verify shapes
        expected_output_shape = (batch_size, info_integration.num_modules, seq_length, hidden_dim)
        self.assert_output_shape(output, expected_output_shape)
    
        # Phi should be a scalar per batch element
>       assert phi.shape == (batch_size,)
E       assert (2, 4) == (2,)
E         
E         Left contains one more item: 4
E         Use -v to get more diff

tests/unit/memory/test_memory_components.py:136: AssertionError
__________________ TestMemoryComponents.test_memory_retention __________________
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

self = <test_memory_components.TestMemoryComponents object at 0x72f1751f06a0>
working_memory = WorkingMemory(
    # attributes
    hidden_dim = 64
    dropout_rate = 0.1
)
key = Array([0, 0], dtype=uint32), batch_size = 2, seq_length = 8
hidden_dim = 64

    def test_memory_retention(self, working_memory, key, batch_size, seq_length, hidden_dim):
        """Test memory retention over sequences."""
        # Create a sequence with a distinctive pattern
        pattern = jnp.ones((batch_size, 1, hidden_dim))
        inputs = jnp.concatenate([
            pattern,
            self.create_inputs(key, batch_size, seq_length-2, hidden_dim),
            pattern
        ], axis=1)
    
        initial_state = jnp.zeros((batch_size, hidden_dim))
    
        input_shape = (batch_size,)
>       variables = working_memory.init(
            key, inputs, initial_state=initial_state, deterministic=True
        )

tests/unit/memory/test_memory_components.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
models/memory.py:102: in __call__
    nn.LSTMCell.initialize_carry(key, input_shape, self.hidden_dim),
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Array([1428664606, 3351135085], dtype=uint32), rng = (2, 64)
input_shape = 64

    @nowrap
    def initialize_carry(
      self, rng: PRNGKey, input_shape: tuple[int, ...]
    ) -> tuple[Array, Array]:
      """Initialize the RNN cell carry.
    
      Args:
        rng: random number generator passed to the init_fn.
        input_shape: a tuple providing the shape of the input to the cell.
      Returns:
        An initialized carry for the given RNN cell.
      """
>     batch_dims = input_shape[:-1]
E     TypeError: 'int' object is not subscriptable

../../.local/lib/python3.10/site-packages/flax/linen/recurrent.py:187: TypeError
=========================== short test summary info ============================
FAILED tests/benchmarks/test_arc_reasoning.py::TestARCReasoning::test_pattern_recognition
FAILED tests/benchmarks/test_arc_reasoning.py::TestARCReasoning::test_abstraction_capability
FAILED tests/benchmarks/test_arc_reasoning.py::TestARCReasoning::test_conscious_adaptation
FAILED tests/benchmarks/test_bigbench_reasoning.py::TestBigBenchReasoning::test_reasoning_capabilities
FAILED tests/benchmarks/test_bigbench_reasoning.py::TestBigBenchReasoning::test_meta_learning
FAILED tests/benchmarks/test_bigbench_reasoning.py::TestBigBenchReasoning::test_consciousness_emergence
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_forward_pass
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_state_initialization
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cross_modal_attention
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_modality_specific_processing
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cognitive_integration
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_phi_metric_computation
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_information_flow
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_memory_integration
FAILED tests/unit/memory/test_memory.py::TestWorkingMemory::test_sequence_processing
FAILED tests/unit/memory/test_memory.py::TestWorkingMemory::test_memory_retention
FAILED tests/unit/memory/test_memory_components.py::TestMemoryComponents::test_memory_sequence_processing
FAILED tests/unit/memory/test_memory_components.py::TestMemoryComponents::test_context_aware_gating
FAILED tests/unit/memory/test_memory_components.py::TestMemoryComponents::test_information_integration
FAILED tests/unit/memory/test_memory_components.py::TestMemoryComponents::test_memory_retention
======================== 23 failed, 28 passed in 14.86s ========================
